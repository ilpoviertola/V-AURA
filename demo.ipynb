{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V-AURA Demo\n",
    "\n",
    "Install the V-AURA environment (see ./README.md) and run the notebook below to generate samples using the V-AURA model.\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import warnings\n",
    "from math import ceil\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from models.vaura_model import VAURAModel\n",
    "from utils.train_utils import get_datamodule_from_type, get_curr_time_w_random_shift\n",
    "from utils.demo_utils import resolve_ckpt_demo, resolve_hparams_demo\n",
    "from models.data.vggsound_dataset import EPS as EPS_VGGSOUND\n",
    "from scripts.generate import save_results, get_original_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "\n",
    "```EXPERIMENT_DIR```: Path to experiment dir. If not provided/not existing default will be downloaded.\n",
    "\n",
    "```AVCLIP_CKPT```: If you have downloaded the Segment AVCLIP checkpoint for VGGSound, provide the path to it. If you have not downloaded the checkpoint, the code will download it for you.\n",
    "\n",
    "```DURATION```: Duration of the audio to be generated. Must not exceed the duration of conditioned video and must be a multiple of 0.64.\n",
    "\n",
    "```STRIDE```: When the context lenght of the model is exceeded (```DURATION``` > 2.56 seconds), stride defains how much of old audio is preserved in the prompt (```DURATION``` - ```STRIDE``` = amount of preserved audio), when generating audio past the 2.56 second mark.\n",
    "\n",
    "```DEVICE```: Cuda is recommended.\n",
    "\n",
    "```TEMP```: Temperature to be used during sampling. 0.95 - 1.0 is recommended.\n",
    "\n",
    "```TOP_K```: Top_k amount for top_k sampling. 128 is recommended.\n",
    "\n",
    "```CFG_SCALE```: Classifier-Free Guidance scale. 6.0 is recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_DIR = \"./logs/24-08-01T08-34-26\"\n",
    "AVCLIP_CKPT = \"./segment_avclip/vggsound/best.pt\"\n",
    "OUTPUT_DIR = \"./demo_output\"\n",
    "\n",
    "DURATION = 2.56  # n * 0.64s\n",
    "STRIDE = 1.28   # n * 0.64s && < 2.56\n",
    "assert DURATION % 0.64 == 0\n",
    "assert STRIDE % 0.64 == 0\n",
    "\n",
    "DEVICE = \"cuda\"\n",
    "TEMP = 0.95\n",
    "TOP_K = 128\n",
    "CFG_SCALE = 6.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolve paths and download checkpoints if needed\n",
    "output_path = Path(OUTPUT_DIR) / f\"generated_samples_{get_curr_time_w_random_shift()}\"\n",
    "output_path.mkdir(exist_ok=True, parents=True)\n",
    "checkpoint_path = resolve_ckpt_demo(EXPERIMENT_DIR)\n",
    "hparams_path = resolve_hparams_demo(checkpoint_path, AVCLIP_CKPT)\n",
    "\n",
    "print(f\"Using checkpoint: {checkpoint_path}\")\n",
    "print(f\"Using hparams: {hparams_path}\")\n",
    "print(f\"Using output path: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "with warnings.catch_warnings():  # :)\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    model = VAURAModel.load_from_checkpoint(\n",
    "        checkpoint_path, hparams_file=hparams_path, map_location=DEVICE\n",
    "    )\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolve dataloader\n",
    "dl_cfg = OmegaConf.load(\"./data/demo/dataloader_config.yaml\")\n",
    "dl_cfg[\"sample_duration\"] = DURATION\n",
    "OmegaConf.resolve(dl_cfg)  # resolve durations\n",
    "\n",
    "datamodule = get_datamodule_from_type(\"motionformer_gen\", dl_cfg)\n",
    "datamodule.setup(\"test\")\n",
    "dataloader = datamodule.test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolve generation parameters\n",
    "MODEL_MAX_DURATION = 2.56  # do not modify\n",
    "COMPRESSION_MODEL_FRAME_RATE = 86  # do not modify\n",
    "if DURATION > MODEL_MAX_DURATION:\n",
    "    assert STRIDE < MODEL_MAX_DURATION\n",
    "\n",
    "total_gen_len = int(DURATION * COMPRESSION_MODEL_FRAME_RATE)\n",
    "stride_tokens = int(STRIDE * COMPRESSION_MODEL_FRAME_RATE)\n",
    "model.sampler.audio_tokens_per_video_frame = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate\n",
    "for sample in tqdm(dataloader):\n",
    "    assert sample[\"meta\"][\"duration\"] >= DURATION, \"Sample duration can not exceed conditional video duration\"\n",
    "    frames = sample[\"frames\"].to(DEVICE)\n",
    "    current_gen_offset: int = 0\n",
    "    prompt_length: int = 0\n",
    "    all_tokens = []\n",
    "    prompt_tokens = None\n",
    "\n",
    "    # get original data without transformations\n",
    "    original_frames, _ = get_original_data(\n",
    "        sample[\"meta\"],\n",
    "        0.0,\n",
    "        EPS_VGGSOUND,\n",
    "        DURATION,\n",
    "        0,\n",
    "    )\n",
    "\n",
    "    start_time = time.time()\n",
    "    if DURATION <= MODEL_MAX_DURATION:  # single chunk generation\n",
    "        item = model.generate(\n",
    "            frames=frames,\n",
    "            audio=prompt_tokens,\n",
    "            max_new_tokens=total_gen_len,\n",
    "            return_sampled_indices=True,\n",
    "            use_sampling=True,\n",
    "            temp=TEMP,\n",
    "            top_k=TOP_K,\n",
    "            cfg_scale=CFG_SCALE,\n",
    "            remove_prompts=False,\n",
    "            prompt_is_encoded=True,\n",
    "        )\n",
    "        generated_audios = item[\"generated_audio\"]\n",
    "\n",
    "    else:  # chunked generation\n",
    "        while current_gen_offset + prompt_length < total_gen_len:\n",
    "            time_offset = current_gen_offset / COMPRESSION_MODEL_FRAME_RATE\n",
    "            chunk_duration = min(DURATION - time_offset, MODEL_MAX_DURATION)\n",
    "            max_gen_len = ceil(chunk_duration * COMPRESSION_MODEL_FRAME_RATE)\n",
    "\n",
    "            # Figure out the frames to use\n",
    "            initial_position = ceil(time_offset * sample[\"meta\"][\"video_fps\"])\n",
    "            video_target_length = ceil(chunk_duration * sample[\"meta\"][\"video_fps\"])\n",
    "            positions = torch.arange(\n",
    "                initial_position // 16,\n",
    "                (initial_position + video_target_length) // 16,\n",
    "                device=DEVICE,\n",
    "            )\n",
    "            selected_frames = frames[:, positions % frames.shape[1], ...]\n",
    "\n",
    "            item = model.generate(\n",
    "                frames=selected_frames,\n",
    "                audio=prompt_tokens,\n",
    "                max_new_tokens=max_gen_len,\n",
    "                return_sampled_indices=True,\n",
    "                use_sampling=True,\n",
    "                temp=TEMP,\n",
    "                top_k=TOP_K,\n",
    "                cfg_scale=CFG_SCALE,\n",
    "                remove_prompts=False,\n",
    "                prompt_is_encoded=True,\n",
    "            )\n",
    "            gen_tokens = item[\"sampled_indices\"]\n",
    "            if prompt_tokens is None:\n",
    "                all_tokens.append(gen_tokens)\n",
    "            else:\n",
    "                all_tokens.append(gen_tokens[:, :, prompt_tokens.shape[-1] :])\n",
    "            prompt_tokens = gen_tokens[:, :, stride_tokens:]\n",
    "            prompt_length = prompt_tokens.shape[-1]\n",
    "            current_gen_offset += stride_tokens\n",
    "\n",
    "        # Gather outputs\n",
    "        gen_tokens = torch.cat(all_tokens, dim=-1)\n",
    "        sampled_frames = [(gen_tokens[..., : model.num_codebooks, :], None)]\n",
    "        generated_audios = model.audio_encoder.decode(sampled_frames)\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Generation took {end_time - start_time:.2f}s\")\n",
    "\n",
    "    # Save results\n",
    "    for i, generated_audios in enumerate(generated_audios):\n",
    "        save_results(\n",
    "            generated_audios[i],\n",
    "            original_frames[i],\n",
    "            output_path,\n",
    "            Path(sample[\"meta\"][\"filepath\"][i]).name,\n",
    "            sample[\"meta\"][\"video_fps\"][i].item(),\n",
    "            44100,\n",
    "            sample[\"meta\"][\"audio_fps\"][i].item(),\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "synchronisonix_cu12.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
